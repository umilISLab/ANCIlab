{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix, ConfusionMatrixDisplay\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import os\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import json\n",
    "from difflib import SequenceMatcher\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy.stats import hmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"mistral\"\n",
    "task = \"multilabel\"\n",
    "approach = \"zero-shot\"\n",
    "tag = \"topic_tags\" if task == \"multilabel\" else \"category_tag\"\n",
    "df = pd.read_json(f\"{task}-{model}-{approach}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_tag(s, possible_tags):\n",
    "\n",
    "    for tag in possible_tags:\n",
    "        if tag in s:\n",
    "            return tag\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = stopwords.words(\"italian\")\n",
    "embedder = SentenceTransformer(\"nickprock/sentence-bert-base-italian-xxl-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def morphological_similarity(a, b, delta_penalty):\n",
    "\n",
    "    a_words = list(filter(lambda w: w not in sw, a.split()))\n",
    "    b_words = list(filter(lambda w: w not in sw, b.split()))\n",
    "    d = []\n",
    "    \n",
    "    for wa in a_words:\n",
    "        d_wa = []\n",
    "        for wb in b_words:\n",
    "            s = SequenceMatcher(None, wa, wb).find_longest_match(0, len(wa), 0, len(wb)).size\n",
    "            s /= np.sqrt(len(wa)*len(wb))\n",
    "            d_wa.append(s)\n",
    "        d.append(d_wa)\n",
    "\n",
    "    D = np.array(d)\n",
    "\n",
    "    if D.shape[0] > D.shape[1]:\n",
    "        delta = D.shape[0] - D.shape[1]\n",
    "        v = D.max(axis = 0)\n",
    "    else:\n",
    "        delta = D.shape[1] - D.shape[0]\n",
    "        v = D.max(axis = 1)\n",
    "\n",
    "    return v.mean() - delta_penalty(delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_tags = df[tag].explode().unique().tolist()\n",
    "df['cleaned_predictions'] = df['predictions'].apply(lambda L: list(map(lambda s: normalize_tag(s, possible_tags), L))).apply(lambda L: [s for s in L if s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_embeddings = embedder.encode(possible_tags, normalize_embeddings=True)\n",
    "semantic_similarity = linear_kernel(tag_embeddings)\n",
    "semantic_map = {k:(k,0) for k in possible_tags}\n",
    "for i in range(len(possible_tags)):\n",
    "    y = possible_tags[i]\n",
    "    for j in range(i+1, len(possible_tags)):\n",
    "        if semantic_similarity[i,j] > semantic_map[y][-1]:\n",
    "            semantic_map[y] = (possible_tags[j], semantic_similarity[i,j])\n",
    "\n",
    "semantic_map = {k:v[0] for k,v in semantic_map.items() if v[1] > 0.6}\n",
    "semantic_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[f'corrected_{tag}'] = df[tag].apply(lambda L: [semantic_map[y] if y in semantic_map else y for y in L])\n",
    "df['corrected_predictions'] = df['cleaned_predictions'].apply(lambda L: [y.strip() for y in L if y]).apply(lambda L: list(set([semantic_map[y] if y in semantic_map else y for y in L])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[f\"corrected_{tag}\", \"corrected_predictions\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if task == \"multiclass\":\n",
    "    df[f\"corrected_{tag}\"] = df[tag].apply(lambda L: L[0])\n",
    "    df['corrected_predictions'] = df['cleaned_predictions'].apply(lambda L: L[0] if L else \"None\")\n",
    "    cm = confusion_matrix(df[f'corrected_{tag}'], df['corrected_predictions'])\n",
    "    ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-correction scores\n",
    "with open(f\"../scores/{task}-{model}-{approach}.json\", \"r\") as f:\n",
    "    scores = json.load(f)\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-correction scores\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_true = mlb.fit_transform(df[f'corrected_{tag}'])\n",
    "y_pred = mlb.transform(df['corrected_predictions'])\n",
    "TP = np.sum(y_true * y_pred, axis = 1)\n",
    "FP = np.sum((1 - y_true)*y_pred, axis = 1)\n",
    "FN = np.sum(y_true*(1-y_pred), axis = 1)\n",
    "new_scores = {\n",
    "    \"macro-precision\": TP.sum()/(TP.sum() + FP.sum()),\n",
    "    \"macro-recall\": TP.sum()/(TP.sum() + FN.sum()),\n",
    "    \"micro-precision\": np.mean(TP / (TP + FP)),\n",
    "    \"micro-recall\": np.mean(TP / (TP + FN))\n",
    "}\n",
    "new_scores['micro-f1'] = hmean([TP / (TP + FP), TP / (TP + FN)], axis = 0).mean()\n",
    "new_scores['macro-f1'] = hmean([new_scores['macro-precision'], new_scores['macro-recall']])\n",
    "new_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computation times\n",
    "times = []\n",
    "for file in os.listdir(\"../scores\"):\n",
    "    with open(os.path.join(\"../scores\", file), \"r\") as f:\n",
    "        d = json.load(f)\n",
    "    file_features = file.split(\".\")[0].split(\"-\")\n",
    "    times.append({\"problem\": file_features[0], \"model\": \"-\".join(file_features[1:-2]), \"approach\": \"-\".join(file_features[-2:]), \"time\": d[\"time\"]})\n",
    "\n",
    "times_df = pd.DataFrame(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(data = times_df, col = \"problem\")\n",
    "g.map(sns.barplot, data = times_df, y = \"approach\", x = \"time\", hue = \"model\", errorbar = None, orient = \"h\", palette = \"rainbow\")\n",
    "g.set_ylabels(\"\")\n",
    "g.set_xlabels(\"Tempo medio (secondi)\")\n",
    "g.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
